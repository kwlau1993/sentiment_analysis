{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "#import nltk package\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#Written by Isil and Lau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)\n",
    "\n",
    "#input date in the formate 'dd-mm-yy'\n",
    "def get_timestamp(date):\n",
    "    tmp = date.split('-')\n",
    "    dt_obj = dt.datetime(int(tmp[2]),int(tmp[1]),int(tmp[0]))\n",
    "    return int(time.mktime(dt_obj.timetuple()))\n",
    "\n",
    "#Sentiment calculation\n",
    "def sentiment_calc(text):\n",
    "    try:\n",
    "        return sia.polarity_scores(text)['compound']\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# apply sentiment calculation to headlines\n",
    "def process_text(headlines):\n",
    "    tokens = []\n",
    "    for line in headlines:\n",
    "        toks = tokenizer.tokenize(line)\n",
    "        toks = [t.lower() for t in toks if t.lower() not in stop_words]\n",
    "        tokens.extend(toks)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters, input date in the formate 'dd-mm-yy'\n",
    "size = \"1000\"\n",
    "start_date = \"17-08-2019\"\n",
    "end_date = \"28-08-2019\"\n",
    "keyword = \"cardano\"\n",
    "\n",
    "# base url and generate timestamp\n",
    "base = \"https://api.pushshift.io/reddit/search/submission/?\"\n",
    "st_date = str(get_timestamp(start_date))\n",
    "en_date = str(get_timestamp(end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment distribution\n",
    "#Divided into positive, negative and neutral sentiment\n",
    "#query data\n",
    "url = base + \"size=\"+size+\"&after=\"+ st_date + \"&before=\" + en_date + \"&subreddit=\" + keyword + \"&sort_type=score&sort=desc\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "#generate dataframe\n",
    "tmp = json_normalize(data, 'data')\n",
    "#cols=['selftext', 'title', 'score', 'num_comments', 'created_utc', 'full_link']\n",
    "cols=['title', 'score', 'created_utc']\n",
    "df2 = pd.DataFrame(tmp, columns=cols)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "#add date\n",
    "_timestamp = df2[\"created_utc\"].apply(get_date)\n",
    "df2 = df2.assign(timestamp = _timestamp)\n",
    "df2['Date'] = pd.to_datetime(df2['timestamp'])\n",
    "tmp = [i.strftime('%m/%Y') for i in df2['Date']]\n",
    "df2['Month'] = pd.Series(tmp)\n",
    "\n",
    "#Sentiment calculation\n",
    "df2['sentiment'] = df2['title'].apply(sentiment_calc)\n",
    "\n",
    "#Add sentiment label, +1 for positive and -1 for negative, 0 for neutral\n",
    "df2['label'] = 0\n",
    "df2.loc[df2['sentiment'] > 0.2, 'label'] = 1\n",
    "df2.loc[df2['sentiment'] < -0.2, 'label'] = -1\n",
    "\n",
    "#Generate csv for the query data\n",
    "df = df2.drop(columns = ['timestamp'])\n",
    "df_sim = df.drop(columns = 'title')\n",
    "file_name = 'Reddit_data' + '_' + keyword + '_' + start_date + '.csv'\n",
    "simp = 'Simp_Reddit_data' + '_' + keyword + '_' + start_date + '.csv'\n",
    "df.to_csv(file_name, index=True)\n",
    "df_sim.to_csv(simp, index=True)\n",
    "\n",
    "#Distribution in python, I shall do this wit sql instead\n",
    "#print(df2.label.value_counts())\n",
    "#print(df2.label.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# obtain top keywords from each sentiment. Can take parameters other than \"most_common\"\n",
    "\n",
    "#Positive headlines. For negative change 1 to -1\n",
    "pos_lines = list(df[df.label == 1].title)\n",
    "pos_tokens = process_text(pos_lines)\n",
    "pos_freq = nltk.FreqDist(pos_tokens)\n",
    "\n",
    "#generate pos_keyword_df\n",
    "tmp = pd.DataFrame({'col':pos_freq.most_common(20)})\n",
    "tmp[['keyword','frequency']] = pd.DataFrame(tmp.col.values.tolist(), index= tmp.index)\n",
    "pos_df = tmp.drop(columns = 'col')\n",
    "\n",
    "#Manuel specification on grouping the full list\n",
    "full_list = list(pos_df['keyword'])\n",
    "ada = ['cardano', 'ada']\n",
    "market = ['crypto', '02btc', 'exchange', 'bitcoin']\n",
    "car_inst = ['charles','hoskinson','emurgo','iohk']\n",
    "car_apps = ['gambling', 'cryptoroulette','free']\n",
    "car_tech = ['platform', 'stake', 'blockchain', 'progress','project', 'marlowe', 'plutus', 'contracts']\n",
    "oth_proj = []\n",
    "not_others = ada + market + car_apps + car_tech\n",
    "others = [i for i in list(pos_df['keyword']) if i not in not_others] \n",
    "\n",
    "#info_array = [ada, market, car_apps, car_tech, others]\n",
    "info_array = [ada, market, car_apps, car_inst, car_tech, oth_proj, others]\n",
    "str_info_array = ['cardano', 'market', 'cardano Dapps', 'cardano institutions', 'cardano technology', 'other blockchain projects', 'other']\n",
    "#frequency count in each group\n",
    "freq_array = [0 for i in range(0,len(info_array))]\n",
    "for j in range(0,len(info_array)):\n",
    "    freq_array[j] = sum([pos_df['frequency'][i] for i in range(0,len(full_list)) if pos_df['keyword'][i] in info_array[j]])\n",
    "\n",
    "pos_grp_freq = pd.DataFrame(freq_array, index = str_info_array, columns = ['frequency'])\n",
    "\n",
    "#naming\n",
    "tot_pos_freq_name = 'Reddit_pos' + '_' + keyword + '_' + start_date + '.csv'\n",
    "pos_freq_name = 'Reddit_pos_grouped' + '_' + keyword + '_' + start_date + '.csv'\n",
    "\n",
    "pos_df.to_csv(tot_pos_freq_name, index=False) \n",
    "pos_grp_freq.to_csv(pos_freq_name)\n",
    "\n",
    "#Positive headlines. For negative change 1 to -1\n",
    "neg_lines = list(df[df.label == -1].title)\n",
    "neg_tokens = process_text(neg_lines)\n",
    "neg_freq = nltk.FreqDist(neg_tokens)\n",
    "\n",
    "#generate neg_keyword_df\n",
    "tmp = pd.DataFrame({'col':neg_freq.most_common(20)})\n",
    "tmp[['keyword','frequency']] = pd.DataFrame(tmp.col.values.tolist(), index= tmp.index)\n",
    "neg_df = tmp.drop(columns = 'col')\n",
    "\n",
    "#Manuel specification on grouping the full list\n",
    "full_list = list(neg_df['keyword'])\n",
    "ada = ['cardano', 'ada']\n",
    "market = ['crypto', '02btc','market','holdings']\n",
    "car_apps = []\n",
    "car_inst = ['emurgo','charles','hoskinson','iohk']\n",
    "car_tech = ['platform','development','project','protocol','contract']\n",
    "oth_proj = ['facebook','consensys','scam','block','craig','eos']\n",
    "not_others = ada + market + car_tech + car_inst\n",
    "others = [i for i in list(neg_df['keyword']) if i not in not_others] \n",
    "\n",
    "info_array = [ada, market, car_apps, car_inst, car_tech, oth_proj, others]\n",
    "str_info_array = ['cardano', 'market', 'cardano Dapps', 'cardano institutions', 'cardano technology', 'other blockchain projects', 'other']\n",
    "#frequency count in each group\n",
    "freq_array = [0 for i in range(0,len(info_array))]\n",
    "for j in range(0,len(info_array)):\n",
    "    freq_array[j] = sum([neg_df['frequency'][i] for i in range(0,len(full_list)) if neg_df['keyword'][i] in info_array[j]])\n",
    "\n",
    "neg_grp_freq = pd.DataFrame(freq_array, index = str_info_array, columns = ['frequency'])\n",
    "\n",
    "#naming\n",
    "tot_neg_freq_name = 'Reddit_neg' + '_' + keyword + '_' + start_date + '.csv'\n",
    "neg_freq_name = 'Reddit_neg_grouped' + '_' + keyword + '_' + start_date + '.csv'\n",
    "\n",
    "neg_df.to_csv(tot_neg_freq_name, index=False) \n",
    "neg_grp_freq.to_csv(neg_freq_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
